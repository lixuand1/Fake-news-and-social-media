{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\n",
      "Requirement already satisfied: h5py in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from protobuf>=3.6.1->tensorflow)\n",
      "Requirement already satisfied: mock>=2.0.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "Requirement already satisfied: h5py in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages (from sklearn)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn: started\n",
      "  Running setup.py bdist_wheel for sklearn: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\dlt\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the vector representations of each word according to Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_path = \"glove.twitter.27B.100d.txt\" ## change\n",
    "# create the word2vec dict from the dictionary\n",
    "def get_word2vec(file_path):\n",
    "    try:\n",
    "        file = open(embedding_path, \"r\", encoding='utf8')\n",
    "        if (file):\n",
    "            word2vec = dict()\n",
    "            split = file.read().splitlines()\n",
    "            for line in split:\n",
    "                key = line.split(' ',1)[0] # the first word is the key\n",
    "                value = np.array([float(val) for val in line.split(' ')[1:]])\n",
    "                word2vec[key] = value\n",
    "            file.close()\n",
    "            return (word2vec)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "w2v = get_word2vec(embedding_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.5849e-01  1.4165e-01  8.3564e-03  4.0577e-02  4.4195e-01 -1.1454e-01\n",
      "  2.0338e-01 -1.3377e-01 -6.1458e-02  1.1220e+00  2.1163e-01 -5.4524e-01\n",
      " -3.0683e+00 -1.4878e-01 -1.4466e-01 -5.9170e-01  9.8828e-03  6.1911e-01\n",
      " -8.6552e-01 -7.0807e-01 -3.6516e-01  2.1672e-02 -3.5861e-01  4.8487e-01\n",
      " -5.6540e-01 -5.3430e-01 -3.4473e-01 -1.9008e-01  9.3164e-02  4.8268e-01\n",
      " -1.8350e-01 -1.1083e-01 -3.3814e-01  6.3688e-01  6.2268e-01  8.2295e-01\n",
      "  4.3433e-01  6.5400e-02  7.2992e-01  2.6747e-02 -8.5995e-01 -3.0779e-02\n",
      " -5.6146e-01  1.8875e-01  7.8385e-01 -2.1126e-01 -2.1223e-01  9.0534e-02\n",
      "  3.2052e-02 -7.1102e-01  9.1668e-01  5.4492e-01 -2.8641e-01  1.3797e-02\n",
      "  8.2005e-01 -2.7975e-02  3.5742e-01 -7.2516e-01 -4.8604e-02 -2.5878e-01\n",
      " -8.5122e-01  1.9878e-01  1.2547e+00  3.9707e-01 -1.4316e-01  1.1500e-02\n",
      " -7.1699e-02 -7.9911e-01  8.2668e-01 -7.4625e-01 -3.7770e-01  1.0727e+00\n",
      " -3.8060e-01  1.9366e-01  2.1991e-01 -5.4513e-01  6.6331e-01 -4.2717e-01\n",
      "  2.5983e-01 -6.0330e-01  9.6712e-01  5.5068e-01  4.8849e-01  7.1383e-01\n",
      "  1.8413e-01 -4.5931e-01 -1.2520e+00 -7.1332e-01 -2.8520e-03 -2.0994e-01\n",
      "  1.3234e-01  5.1256e-01  1.8108e-01  2.1557e-01 -1.2466e-01 -3.3489e-01\n",
      "  8.7809e-01  4.7777e-01  2.1673e-01  1.6425e-01]\n"
     ]
    }
   ],
   "source": [
    "print(w2v['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dlt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "stopwords = {'then', 'to', 'himself', 'you', 'as', 'it', 'am', 'whom', 'was', \"you're\", 'll', 'once', 'from', 'does', 's', 'nor', 'their',\n",
    "             'ma', 'a', 'while', 'any', 'yours', 'itself', 'during', 'by', 'where', 'its', 'on', 'no', 'yourselves', 'she', 'here', 'myself',\n",
    "             'do', 'about', 'can', 'we', 'm', 'and', 'there', 'should', 'who', 'those', 'these', 'through', 't', 'below', 'what', 'your',\n",
    "             'how', 'just', 'down', 'own', 'y', 'hers', 'is', 'so', 'being', 'the', \"you've\", \"it's\", 'more', 're', 'them', 'have', 'off',\n",
    "             'been', 'him', 'this', 'will', 'each', 'all', 'herself', 'up', 'has', 'ours', 'because', \"you'll\", \"you'd\", 'his', 'very', 'are',\n",
    "             'having', 'which', 'that', 'into', 'if', 'themselves', 'of', 'theirs', 'for', 'out', 'd', 'yourself', 'o', 'they', 'our', 'over',\n",
    "             'he', 've', 'doing', 'some', \"that'll\", 'now', 'were', 'than', 'ourselves', 'above', 'did', 'under', 'me', 'her', 'until','my',\n",
    "             'further', 'had', 'other', \"she's\", 'an', 'between', 'but', 'such', 'with', 'be', 'i', 'at', 'when', 'why', 'or', 'before', 'in'}\n",
    "\n",
    "\n",
    "def get_tokens(sentence):\n",
    "    tokens = tknzr.tokenize(sentence)\n",
    "    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_lemma(word): # lemmatize and lower the word\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word.lower()\n",
    "    else:\n",
    "        return lemma.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training text as input and label as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "source_file_name = 'test_c.json'\n",
    "source_file = open(source_file_name,'r', encoding='utf8')\n",
    "source_sentences = []\n",
    "true_sentences = []\n",
    "false_sentences = []\n",
    "labels = []\n",
    "for line in source_file:\n",
    "    source_json = json.loads(line)    \n",
    "    if source_json['text']:\n",
    "        if source_json['label'] == \"0\":\n",
    "            true_sentences.append(source_json['text'])\n",
    "        else:\n",
    "            false_sentences.append(source_json['text'])\n",
    "        source_sentences.append(source_json['text'])\n",
    "        labels.append(source_json['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620000\n",
      "310000\n",
      "310000\n"
     ]
    }
   ],
   "source": [
    "print(len(source_sentences))\n",
    "print(len(true_sentences))\n",
    "print(len(false_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Credbank test set input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_set(file_name):\n",
    "    test_list = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_list.append(line.replace('\\n','').lower())\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list_cred = get_test_set('cred_realnews_processed.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pheme test set input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pheme_set(file_name):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            text_list.append(line['text'].lower())\n",
    "            label_list.append(line['label'])\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list_pheme, label_list_pheme = get_pheme_set('pheme_processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are no verses in the quran about me wanting anyone to hold hostages in a chocolate shop in sydney, you terrorist fucks.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_list_pheme[0])\n",
    "print(label_list_pheme[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Combined train and test set input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_combined_set(file_name1, file_name2, num1, num2):\n",
    "#     test_list = []\n",
    "#     with open(file_name1, 'r', encoding='utf-8') as f1:\n",
    "#         i=0\n",
    "#         if i < num1:\n",
    "#         for line in f1:\n",
    "#             source_json = json.loads(line)    \n",
    "#             if source_json['text']:\n",
    "#                 source_sentences.append(source_json['text'])\n",
    "#                 labels.append(source_json['label'])\n",
    "#     with open(file_name2, 'r', encoding='utf-8') as f2:\n",
    "#         i=0\n",
    "#         for line in f2:\n",
    "#             if i < num:\n",
    "#                 test_list.append(line.replace('\\n','').lower())\n",
    "#             if i == num2 - 1:\n",
    "#                 break\n",
    "#             i += 1\n",
    "#     return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_set(real_texts1, real_texts2, fake_texts, real_num , fake_num):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    train_y = []\n",
    "    test_y = []\n",
    "    test_set += real_texts1[:len(real_texts1)-real_num]\n",
    "    print(\"faknewsnet real test: \", len(test_set))\n",
    "    test_set += real_texts2[:real_num]\n",
    "    print(\"real test: \", len(test_set))\n",
    "    test_set += fake_texts[:fake_num]\n",
    "    print(\"test: \", len(test_set))\n",
    "    test_y = [0] * len(real_texts1) + [1]* fake_num\n",
    "    train_set += real_texts1[len(real_texts1)-real_num:]\n",
    "    print(\"fakenewsnet train real: \", len(train_set))\n",
    "    train_set += real_texts2[real_num:]\n",
    "    print(\"real train: \",len(train_set))\n",
    "    train_set += fake_texts[fake_num:]\n",
    "    train_y = [0] * len(real_texts2) + [1]* (len(fake_texts)-fake_num)\n",
    "\n",
    "    return train_set, test_set, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310000\n",
      "499277\n",
      "310000\n"
     ]
    }
   ],
   "source": [
    "print(len(true_sentences))\n",
    "print(len(test_list_cred))\n",
    "print(len(false_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faknewsnet real test:  160000\n",
      "real test:  310000\n",
      "test:  460000\n",
      "fakenewsnet train real:  150000\n",
      "real train:  499277\n"
     ]
    }
   ],
   "source": [
    "train_combined_ordered,test_combined_ordered,train_y_co,test_y_co = get_combined_set(true_sentences, test_list_cred, false_sentences , 150000, 150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659277\n",
      "460000\n",
      "659277\n",
      "460000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_combined_ordered))\n",
    "print(len(test_combined_ordered))\n",
    "print(len(train_y_co))\n",
    "print(len(test_y_co))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the inputs and output labels randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_dict = {}\n",
    "for i in range(len(source_sentences)):\n",
    "    training_dict[source_sentences[i]] = labels[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "training_list = list(training_dict.items())\n",
    "random.shuffle(training_list)\n",
    "\n",
    "processed_texts, processed_labels = [], []\n",
    "for text, label in training_list:\n",
    "    processed_texts.append(text.rstrip())\n",
    "    processed_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Tweets text and make it a token library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "for text in processed_texts:\n",
    "    token_list += get_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6376498\n",
      "['scary', 'scary', 'stuff', 'wake', 'america', 'wake', 'people', 'common', 'sense', 'hit', 'right', 'face', 'don', 'ignore', 'wi', 'news', 'income', 'poverty', 'health', 'insurance', 'coverage', 'united', 'states', 'this', 'report', 'present', 'data', 'sorry', 'vicki', 'say', 'us', 'something', 'else', 'please', 'don', 'leave', 'make', 'show', 'breakingnews', 'princess', 'charlotte', 'net', 'worth', 'is', 'more', 'than', 'billion', 'more', 'than', 'prince', 'george', 'kanye', 'west', 'mysteriously', 'delete', 'twitter', 'instagram', 'account', 'samheughan', 'alphamalemadness', 'wait', 'til', 'get', 'home', 'holiday', 'binge', 'watch', 'shadowhunters', 'netflix', 'aus', 'superpumped', 'bookishthings', 'chris', 'pratt', 'dating', 'mystery', 'blonde', 'gossip', 'cop', 'hell', 'bell', 'sir', 'one', 'hundred', 'percent', 'correct', 'damnit', 'janet', 'breaking', 'tom', 'petty', 'reportedly', 'rush', 'hospital', 'full', 'cardiac', 'arrest', 'after', 'suffering', 'heart']\n"
     ]
    }
   ],
   "source": [
    "print(len(token_list))\n",
    "print(token_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the average, max, mid length for a more precise padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617418\n",
      "16.334356951044512\n",
      "75\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "sent_len_list = []\n",
    "for sent in processed_texts:\n",
    "    sent_len_list.append(sent.count(' '))\n",
    "sent_len_list = sorted(sent_len_list)\n",
    "print(len(sent_len_list))\n",
    "print(sum(sent_len_list)/len(sent_len_list))\n",
    "print(max(sent_len_list))\n",
    "print(sent_len_list[int(len(sent_len_list)/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sent_len_list[15000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode sentences using the tokenized document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# prepare tokenizer\n",
    "t = Tokenizer(oov_token=\"<UKN>\")\n",
    "t.fit_on_texts(token_list)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(processed_texts)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 50\n",
    "X = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_testset = t.texts_to_sequences(test_list)\n",
    "X_test = pad_sequences(encoded_testset, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_testset_pheme = t.texts_to_sequences(test_list_pheme)\n",
    "X_test_pheme = pad_sequences(encoded_testset_pheme, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_true = t.texts_to_sequences(true_sentences)\n",
    "X_true = pad_sequences(encoded_true, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_false = t.texts_to_sequences(false_sentences)\n",
    "X_false = pad_sequences(encoded_false, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_train_combined_ordered = t.texts_to_sequences(train_combined_ordered)\n",
    "X_train_co = pad_sequences(encoded_train_combined_ordered, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_test_combined_ordered = t.texts_to_sequences(test_combined_ordered)\n",
    "X_test_co = pad_sequences(encoded_test_combined_ordered, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558000\n",
      "561277\n",
      "558000\n",
      "561277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y_true = [0]*len(X_true)\n",
    "Y_test_cred = [0]*len(X_test) \n",
    "Y_false = [1]*len(X_false)\n",
    "X_true_train1,X_true_test1, Y_true_train1, Y_true_test1 =  train_test_split(X_true, Y_true,test_size =0.5,random_state= 4 )\n",
    "X_true_train2,X_true_test2, Y_true_train2, Y_true_test2 =  train_test_split(X_test, Y_test_cred,test_size =0.5,random_state= 4 )\n",
    "X_false_train,X_false_test, Y_false_train, Y_false_test =  train_test_split(X_false, Y_false,test_size =0.2,random_state= 4 )\n",
    "X_combined_train= np.concatenate((X_true_train1, X_true_train2[:int((len(X_true)/2))]), axis=0)\n",
    "X_combined_train= np.concatenate((X_combined_train, X_false_train), axis=0)\n",
    "print(len(X_combined_train))\n",
    "X_combined_test= np.concatenate((X_true_test1, X_true_train2[int((len(X_true)/2)):]), axis=0)\n",
    "X_combined_test= np.concatenate((X_combined_test, X_true_test2 ), axis=0)\n",
    "X_combined_test= np.concatenate((X_combined_test, X_false_test ), axis=0)\n",
    "print(len(X_combined_test))\n",
    "Y_combined_train= np.concatenate((Y_true_train1, Y_true_train2[:int((len(X_true)/2))]), axis=0)\n",
    "Y_combined_train= np.concatenate((Y_combined_train, Y_false_train), axis=0)\n",
    "print(len(Y_combined_train))\n",
    "Y_combined_test= np.concatenate((Y_true_test1, Y_true_train2[int((len(X_true)/2)):] ), axis=0)\n",
    "Y_combined_test= np.concatenate((Y_combined_test, Y_true_test2  ), axis=0)\n",
    "Y_combined_test= np.concatenate((Y_combined_test, Y_false_test ), axis=0)\n",
    "print(len(Y_combined_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(X_test_pheme))\n",
    "print(X_test_pheme[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617418\n",
      "[  238     1  2154   852  2460  3991   125  1273   182 10942  7610  6339\n",
      "    17  2154   852     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "499277\n",
      "[  137   467  1425  1234 13188   571   928    56   194     2  4009  5307\n",
      "     5  1602     1  7531    40     1     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "7878\n",
      "[ 162    2    1 6395    3  345  264 7226   26  624  698   40    2  770\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(X[10])\n",
    "print(len(X_test))\n",
    "print(X_test[10])\n",
    "print(len(X_test_pheme))\n",
    "print(X_test_pheme[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform label to unique number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "Y_new = processed_labels\n",
    "Y_new = le.fit_transform(Y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0\n",
      " 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_new[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pheme = label_list_pheme\n",
    "Y_pheme = le.fit_transform(Y_pheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pheme[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_co = train_y_co\n",
    "Y_train_co = le.fit_transform(Y_train_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659277\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train_co))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Word2Vec representations of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the embedding matrix from the embedding layer\n",
    "from numpy import zeros\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = w2v.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_matrix[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(?, 50), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Tensor(\"embedding_1/embedding_lookup/Identity:0\", shape=(?, 50, 100), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 200), dtype=float32)\n",
      "Tensor(\"time_distributed_1/Reshape_1:0\", shape=(?, 50, 100), dtype=float32)\n",
      "Tensor(\"flatten_1/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"dense_2/Relu:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"dense_3/Softmax:0\", shape=(?, 3), dtype=float32)\n",
      "<keras.engine.training.Model object at 0x000001E9FF47C780>\n"
     ]
    }
   ],
   "source": [
    "# main model\n",
    "from keras import Input, Model\n",
    "from keras.layers import Bidirectional, Embedding, TimeDistributed, Dense, LSTM, Flatten\n",
    "\n",
    "inpt = Input(shape=(max_length,))\n",
    "print(inpt)\n",
    "model = Embedding(vocab_size,100, weights=[embedding_matrix],input_length=max_length)(inpt)\n",
    "# Embedding Layer: input_dimension=vocabulary size, output_dimension(to dense layer,should be consistent with embedding matrix?),\n",
    "# weights using pretrained embeddings, max length of tokenized sentence input)\n",
    "print(model)\n",
    "model = Bidirectional(LSTM (100,return_sequences=True,dropout=0.50),merge_mode='concat')(model)\n",
    "# LSTM: units=100->output dimension to next layer, dropout->prevent overfitting, \n",
    "print(model)\n",
    "model = TimeDistributed(Dense(100,activation='relu'))(model) \n",
    "# activation function -> transform weighted sum to activation, Rectified Linear Unit overcomes the vanishing gradient problem\n",
    "print(model)\n",
    "model = Flatten()(model)\n",
    "print(model)\n",
    "model = Dense(100,activation='relu')(model)\n",
    "print(model)\n",
    "outpt = Dense(3,activation='softmax')(model)\n",
    "print(outpt)\n",
    "model = Model(inpt,outpt)\n",
    "print(model)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_ev, Y_train, Y_ev =  train_test_split(X, Y_new,test_size =0.1,random_state= 4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  233  1615   412 ...     0     0     0]\n",
      " [21323   927  2259 ...     0     0     0]\n",
      " [  328    82     2 ...     0     0     0]\n",
      " ...\n",
      " [21323 32431   107 ...     0     0     0]\n",
      " [    2   164  1217 ...     0     0     0]\n",
      " [ 3208    31   325 ...     0     0     0]]\n",
      "[1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0\n",
      " 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1\n",
      " 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1]\n",
      "[[9889 1602   43 ...    0    0    0]\n",
      " [2437 5096   76 ...    0    0    0]\n",
      " [ 893  856  345 ...    0    0    0]\n",
      " ...\n",
      " [ 858  858  858 ...    0    0    0]\n",
      " [  34  321   40 ...    0    0    0]\n",
      " [ 693  318 6872 ...    0    0    0]]\n",
      "[0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0\n",
      " 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0\n",
      " 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:100])\n",
    "print(Y_train[:100])\n",
    "print(X_ev[:100])\n",
    "print(Y_ev[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 593349 samples, validate on 65928 samples\n",
      "Epoch 1/1\n",
      " - 3592s - loss: 0.0652 - acc: 0.9728 - val_loss: 0.6029 - val_acc: 0.7434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea4a64a358>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_co,Y_train_co,validation_split=0.1, nb_epoch = 1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.729001\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_ev, Y_ev, verbose=2)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Y_ev_pred = model.predict(X_ev)\n",
    "Y_ev_pred = np.array([np.argmax(pred) for pred in Y_ev_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61742\n",
      "61742\n",
      "0\n",
      "<class 'list'>\n",
      "[[29296  1703]\n",
      " [  934 29809]]\n",
      "0.969103539530268 0.945957095709571 0.9450627439594825 0.9696191002829913 0.9569321726632805 0.9576419564693599\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_ev_pred))\n",
    "print(len(Y_ev))\n",
    "print(Y_ev[0])\n",
    "print(type(Y_ev))\n",
    "conf_matrix = confusion_matrix(Y_ev, Y_ev_pred)\n",
    "print(conf_matrix)\n",
    "precision0 = conf_matrix[0][0]/(conf_matrix[0][0] +conf_matrix[1][0])\n",
    "precision1 = conf_matrix[1][1]/(conf_matrix[0][1] +conf_matrix[1][1])\n",
    "recall0 = conf_matrix[0][0]/(conf_matrix[0][0] +conf_matrix[0][1])\n",
    "recall1 = conf_matrix[1][1]/(conf_matrix[1][0] +conf_matrix[1][1])\n",
    "f1_score0 = 2 * (precision0 * recall0) / (precision0 + recall0)\n",
    "f1_score1 = 2 * (precision1 * recall1) / (precision1 + recall1)\n",
    "print(precision0, precision1, recall0, recall1, f1_score0, f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict credbank test set label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = np.array([np.argmax(pred) for pred in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36064749627962034\n"
     ]
    }
   ],
   "source": [
    "true_count = 0\n",
    "for label in Y_test:\n",
    "    if label == 0:\n",
    "        true_count += 1\n",
    "print(true_count/len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_pheme = model.predict(X_test_pheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_pheme = np.array([np.argmax(pred) for pred in Y_test_pheme])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7878\n",
      "7878\n",
      "1\n",
      "[[ 845 3094]\n",
      " [1122 2817]]\n",
      "0.4295882053889171 0.476569108441888 0.2145214521452145 0.7151561309977151 0.2861496782932611 0.5719796954314721\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_test_pheme))\n",
    "print(len(Y_pheme))\n",
    "print(Y_test_pheme[0])\n",
    "conf_matrix = confusion_matrix(Y_pheme, Y_test_pheme)\n",
    "print(conf_matrix)\n",
    "precision0 = conf_matrix[0][0]/(conf_matrix[0][0] +conf_matrix[1][0])\n",
    "precision1 = conf_matrix[1][1]/(conf_matrix[0][1] +conf_matrix[1][1])\n",
    "recall0 = conf_matrix[0][0]/(conf_matrix[0][0] +conf_matrix[0][1])\n",
    "recall1 = conf_matrix[1][1]/(conf_matrix[1][0] +conf_matrix[1][1])\n",
    "f1_score0 = 2 * (precision0 * recall0) / (precision0 + recall0)\n",
    "f1_score1 = 2 * (precision1 * recall1) / (precision1 + recall1)\n",
    "print(precision0, precision1, recall0, recall1, f1_score0, f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict combined test set label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_combine_pred = model.predict(X_combined_test)\n",
    "Y_combine_pred = np.array([np.argmax(pred) for pred in Y_combine_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561277\n",
      "561277\n",
      "0\n",
      "[[485624  13653]\n",
      " [  3524  58476]]\n",
      "0.9927956364944761 0.8107141371709021 0.9726544583467694 0.9431612903225807 0.9826218478893188 0.8719367176374984\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_combine_pred))\n",
    "print(len(Y_combined_test))\n",
    "print(Y_combine_pred[0])\n",
    "conf_matrix = confusion_matrix(Y_combined_test, Y_combine_pred)\n",
    "print(conf_matrix)\n",
    "precision0 = conf_matrix[0][0]/(conf_matrix[0][0] +conf_matrix[1][0])\n",
    "precision1 = conf_matrix[1][1]/(conf_matrix[0][1] +conf_matrix[1][1])\n",
    "recall0 = conf_matrix[0][0]/(conf_matrix[0][0] +conf_matrix[0][1])\n",
    "recall1 = conf_matrix[1][1]/(conf_matrix[1][0] +conf_matrix[1][1])\n",
    "f1_score0 = 2 * (precision0 * recall0) / (precision0 + recall0)\n",
    "f1_score1 = 2 * (precision1 * recall1) / (precision1 + recall1)\n",
    "print(precision0, precision1, recall0, recall1, f1_score0, f1_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred_co = model.predict(X_test_co[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9714225  0.02857753 0.        ]\n",
      " [0.23486398 0.76513594 0.        ]\n",
      " [0.09289969 0.9071003  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred_co[:3])\n",
    "Y_pred_co = np.array([np.argmax(pred) for pred in Y_pred_co])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460000\n",
      "460000\n",
      "0\n",
      "[[306856   3144]\n",
      " [ 14690 135310]]\n",
      "0.9543144682253861 0.9772920970141707 0.989858064516129 0.9020666666666667 0.9717613602176247 0.9381738509433046\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_pred_co))\n",
    "print(len(test_y_co))\n",
    "print(Y_pred_co[0])\n",
    "conf_matrix = confusion_matrix(test_y_co, Y_pred_co)\n",
    "print(conf_matrix)\n",
    "precision0 = conf_matrix[0][0]/(conf_matrix[0][0] +conf_matrix[1][0])\n",
    "precision1 = conf_matrix[1][1]/(conf_matrix[0][1] +conf_matrix[1][1])\n",
    "recall0 = conf_matrix[0][0]/(conf_matrix[0][0] +conf_matrix[0][1])\n",
    "recall1 = conf_matrix[1][1]/(conf_matrix[1][0] +conf_matrix[1][1])\n",
    "f1_score0 = 2 * (precision0 * recall0) / (precision0 + recall0)\n",
    "f1_score1 = 2 * (precision1 * recall1) / (precision1 + recall1)\n",
    "print(precision0, precision1, recall0, recall1, f1_score0, f1_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
